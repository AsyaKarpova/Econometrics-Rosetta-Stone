# Коан о простой линейной регрессии {#simplereg}


```{r setup, include=FALSE}
library(knitr)
library(texreg)
library(Statamarkdown)
library(reticulate)
stataexe <- find_stata()
knitr::opts_chunk$set(engine.path = list(stata = stataexe), collectcode = TRUE)
```

Построим простую линейную регрессию в R и проведем несложные тесты. 

Загрузим необходимые пакеты и импортируем данные.
```{r "library chunk",  message=FALSE, warning=FALSE}
library(tidyverse) # для манипуляций с данными и построения графиков
library(skimr) #для красивого summary
library(rio) # для чтения .dta файлов
library(car) # для линейных гипотез
library(tseries)# для теста на нормальность
df = import(file = "us-return.dta")
```

Исследуем наш датасет.

```{r "rename chunk", results = 'hide', message=FALSE, warning=FALSE}
#skim_with(numeric = list(hist = NULL))
skim(df) #посмотрим на наши данные 
df = na.omit(df) # избавимся от пропущенных значений
df = rename(df, n = A, date = B) # дадим столбцам осмысленные названия :)
```

Будем верить в CAPM :) Оценим параметры модели для компании MOTOR. Соответственно, зависимая переменная - разница доходностей акций MOTOR и безрискового актива, а регрессор - рыночная премия.
```{r "mutate"}
#создаем новые переменные и добавляем их к набору данных
df <- mutate(df, y = MOTOR - RKFREE, x = MARKET - RKFREE) 
```

Визуализируем зависимость регрессора и зависимой переменной.
```{r "plot"}
ggplot(df, aes(x = x, y = y)) + geom_point() + geom_smooth(method=lm) +
labs(x = "risk premium", y = "return")
```

Строим нашу модель и проверяем гипотезу об адекватности регрессии.
```{r "model"}
ols <- lm(y ~ x, data = df) 
summary(ols)
```

Проверим гипотезу о равенстве коэффициента при регрессии единице. 
```{r "lin hyp"}
linearHypothesis(ols, c("(Intercept) = 0", "x = 1"))
```

Сделаем предсказание по выборке.
```{r "prediction"}
df <- mutate(df, u_hat = resid(ols), 
             y_hat = predict(ols), 
             n = seq(dim(df)[1]))
```

Посмотрим на остатки :) Протестируем остатки регрессии на нормальность с помощью теста Харке-Бера.
```{r}
jarque.bera.test(df$u_hat)
```

И тест Шапиро-Уилка.
```{r}
shapiro.test(df$u_hat)
```

Оба теста указывают на нормальность распределения остатков регрессии.
