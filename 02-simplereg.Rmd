# Коан о простой линейной регрессии {#simplereg}


```{r setup, include=FALSE}
library(knitr)
library(texreg)
library(Statamarkdown)
library(reticulate)
stataexe <- find_stata()
stataexe = "C:/Program Files (x86)/Stata13/StataMP-64.exe"
knitr::opts_chunk$set(engine.path = list(stata = stataexe), collectcode = TRUE)
```

Построим простую линейную регрессию в R и проведем несложные тесты. 

Загрузим необходимые пакеты.
```{r "library chunk", results='hide', message=FALSE, warning=FALSE}
library(tidyverse) # для манипуляций с данными и построения графиков
library(skimr) #для красивого summary
library(rio) # для чтения .dta файлов
library(car) # для линейных гипотез
library(tseries)# для теста на нормальность
library(sjPlot)
```

Импортируем данные.
```{r "import data", message=FALSE, warning=FALSE}
df = import(file = "us-return.dta")
```

Исследуем наш датасет.

```{r "skim",  message=FALSE, warning=FALSE}
skim_with(numeric = list(hist = NULL, p25 = NULL, p75 = NULL)) #посмотрим на данные
skim(df)
```

```{r "rename chunk",  message=FALSE, warning=FALSE}
df = rename(df, n = A, date = B) # дадим столбцам более осмысленные названия
```

Будем верить в CAPM :) Оценим параметры модели для компании MOTOR. Соответственно, зависимая переменная - разница доходностей акций MOTOR и безрискового актива, а регрессор - рыночная премия.
```{r "mutate", warning=FALSE}
#создаем новые переменные и добавляем их к набору данных
df = mutate(df, y = MOTOR - RKFREE, x = MARKET - RKFREE) 
```

Строим нашу модель и проверяем гипотезу об адекватности регрессии.
```{r "model", message=FALSE, warning=FALSE}
ols <- lm(y ~ x, data = df)
plot_residuals(ols, geom.size = 2,
  show.lines = TRUE, show.resid = TRUE, show.pred = TRUE,
  show.ci = TRUE)
summary(ols)
```

Строим доверительный интервал для параметров модели.
```{r "ci", warning=FALSE}
est = cbind(Estimate = coef(ols), confint(ols))
```

Проверим гипотезу о равенстве коэффициента при регрессии единице. 
```{r "lin hyp"}
linearHypothesis(ols, c("(Intercept) = 0", "x = 1"))
```

Посмотрим на остатки :) Протестируем остатки регрессии на нормальность с помощью теста Харке-Бера.

$H_{0}: S = 0, K = 3$

```{r}
jarque.bera.test(resid(ols)) ##в R нет поправки на размер выборки
```

И тест Шапиро-Уилка.
$H_{0}: \epsilon_{i} ~ N(\mu,\sigma^2)$
```{r}
shapiro.test(resid(ols))
```

Оба теста указывают на нормальность распределения остатков регрессии.

Сделаем прогноз модели по данным вне обучаемо выборки.
```{r "prediction"}
newData = data.frame(x_new = df$x+0.5*rnorm(length(df$x)))
skim(newData)
yhat=predict(ols, newdata = newData, type = "response", se = TRUE)
```


#### То же самое в стате

Загружаем данные. 
```{stata}
use us-return.dta
```


Любуемся и даем новые названия столбцам.
```{stata}
summarize
ren A n
ren B date
```

Убраем пропущенные значения и создаем новые переменные.
```{r "new var", eval=FALSE}
drop if n==.
gen y=MOTOR-RKFREE
gen x=MARKET-RKFREE
```

Визуализируем зависимость.
```{r "plot_stata", eval=FALSE}
graph twoway (lfit y x) (scatter y x)
```

Строим модель и проверяем гипотезу об адекватности регрессии.
```{r "model_stata", eval = FALSE}
reg y x
```

Проверим гипотезу ....
```{r "lin hyp_stata", eval=FALSE}
test _cons=0 x=1
```

Сделаем предсказание по выборке.
```{r "prediction_stata", eval=FALSE}
predict u_hat, resid
predict y_hat
```

Протестируем остатки регрессии на нормальность с помощью теста Харке-Бера.
```{r "Jarque–Bera", eval=FALSE}
sktest u_hat, noadjust #но все равно не как в R
```

И тест Шапиро-Уилка.
```{r "Shapiro-Wilk", eval=FALSE}
swilk u_hat
```

регрессор - uhat
регрессор - uhat^2
uhat^(n-1) - uhat^(n)
дигональные элементы матрицы шляпницы - для выбросов