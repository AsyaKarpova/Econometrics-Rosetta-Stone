# Коан о простой линейной регрессии {#simplereg}




Построим простую линейную регрессию в R и проведем несложные тесты. 

Загрузим необходимые пакеты.

```r
library(tidyverse) # для манипуляций с данными и построения графиков
library(skimr) #для красивого summary
library(rio) # для чтения .dta файлов
library(car) # для линейных гипотез
library(tseries)# для теста на нормальность
library(sjPlot)
```

Импортируем данные.

```r
df = import(file = "us-return.dta")
```

Исследуем наш датасет.


```r
skim_with(numeric = list(hist = NULL, p25 = NULL, p75 = NULL)) #посмотрим на данные
skim(df)
```

```
Skim summary statistics
 n obs: 2664 
 n variables: 22 

-- Variable type:character ------------------------------------------------------------------------------------------------------------------
 variable missing complete    n min max empty n_unique
        B       0     2664 2664   0   6  2544       31

-- Variable type:numeric --------------------------------------------------------------------------------------------------------------------
 variable missing complete    n    mean      sd      p0     p50    p100
        A    2544      120 2664 60.5    34.79    1      60.5    120    
    BOISE    2544      120 2664  0.017   0.097  -0.27    0.015    0.38 
   CITCRP    2544      120 2664  0.012   0.081  -0.28    0.011    0.32 
    CONED    2544      120 2664  0.019   0.05   -0.14    0.019    0.15 
   CONTIL    2544      120 2664 -0.0011  0.15   -0.6     0        0.97 
   DATGEN    2544      120 2664  0.0075  0.13   -0.34    0.017    0.53 
      DEC    2544      120 2664  0.02    0.099  -0.36    0.024    0.38 
    DELTA    2544      120 2664  0.012   0.096  -0.26    0.013    0.29 
   GENMIL    2544      120 2664  0.017   0.065  -0.15    0.011    0.19 
   GERBER    2544      120 2664  0.016   0.088  -0.29    0.015    0.23 
      IBM    2544      120 2664  0.0096  0.059  -0.19    0.002    0.15 
   MARKET    2544      120 2664  0.014   0.068  -0.26    0.012    0.15 
    MOBIL    2544      120 2664  0.016   0.08   -0.18    0.012    0.37 
    MOTOR    2544      120 2664  0.018   0.097  -0.33    0.016    0.27 
    PANAM    2544      120 2664  0.0035  0.13   -0.31    0        0.41 
     PSNH    2544      120 2664 -0.0042  0.11   -0.48    0        0.32 
   rkfree    2544      120 2664  0.0068  0.0022  0.0021  0.0066   0.013
   RKFREE    2544      120 2664  0.0068  0.0022  0.0021  0.0066   0.013
    TANDY    2544      120 2664  0.025   0.13   -0.25    0.022    0.45 
   TEXACO    2544      120 2664  0.012   0.08   -0.19    0.01     0.4  
    WEYER    2544      120 2664  0.0096  0.085  -0.27   -0.002    0.27 
```


```r
df = rename(df, n = A, date = B) # дадим столбцам более осмысленные названия
```

Будем верить в CAPM :) Оценим параметры модели для компании MOTOR. Соответственно, зависимая переменная - разница доходностей акций MOTOR и безрискового актива, а регрессор - рыночная премия.

```r
#создаем новые переменные и добавляем их к набору данных
df = mutate(df, y = MOTOR - RKFREE, x = MARKET - RKFREE) 
```

Строим нашу модель и проверяем гипотезу об адекватности регрессии.

```r
ols <- lm(y ~ x, data = df)
plot_residuals(ols, geom.size = 2,
  show.lines = TRUE, show.resid = TRUE, show.pred = TRUE,
  show.ci = TRUE)
```

<img src="02-simplereg_files/figure-html/model-1.png" width="672" />

```r
summary(ols)
```

```

Call:
lm(formula = y ~ x, data = df)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.168421 -0.059381 -0.003399  0.061373  0.182991 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 0.005253   0.007200   0.730    0.467    
x           0.848150   0.104814   8.092 5.91e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.07844 on 118 degrees of freedom
  (2544 observations deleted due to missingness)
Multiple R-squared:  0.3569,	Adjusted R-squared:  0.3514 
F-statistic: 65.48 on 1 and 118 DF,  p-value: 5.913e-13
```

Строим доверительный интервал для параметров модели.

```r
est = cbind(Estimate = coef(ols), confint(ols))
```

Проверим гипотезу о равенстве коэффициента при регрессии единице. 

```r
linearHypothesis(ols, c("(Intercept) = 0", "x = 1"))
```

```
Linear hypothesis test

Hypothesis:
(Intercept) = 0
x = 1

Model 1: restricted model
Model 2: y ~ x

  Res.Df     RSS Df Sum of Sq      F Pr(>F)
1    120 0.74108                           
2    118 0.72608  2  0.014998 1.2187 0.2993
```

Посмотрим на остатки :) Протестируем остатки регрессии на нормальность с помощью теста Харке-Бера.

$H_{0}: S = 0, K = 3$


```r
jarque.bera.test(resid(ols)) ##в R нет поправки на размер выборки
```

```

	Jarque Bera Test

data:  resid(ols)
X-squared = 1.7803, df = 2, p-value = 0.4106
```

И тест Шапиро-Уилка.
$H_{0}: \epsilon_{i} ~ N(\mu,\sigma^2)$

```r
shapiro.test(resid(ols))
```

```

	Shapiro-Wilk normality test

data:  resid(ols)
W = 0.99021, p-value = 0.5531
```

Оба теста указывают на нормальность распределения остатков регрессии.

Сделаем прогноз модели по данным вне обучаемо выборки.

```r
newData = data.frame(x_new = df$x+0.5*rnorm(length(df$x)))
skim(newData)
```

```
Skim summary statistics
 n obs: 2664 
 n variables: 1 

-- Variable type:numeric --------------------------------------------------------------------------------------------------------------------
 variable missing complete    n    mean  sd    p0   p50 p100
    x_new    2544      120 2664 -0.0024 0.5 -1.07 0.046 1.22
```

```r
yhat=predict(ols, newdata = newData, type = "response", se = TRUE)
```

```
Error in eval(predvars, data, env): объект 'x' не найден
```


#### То же самое в стате

Загружаем данные. 

```stata
use us-return.dta
```

``````


Любуемся и даем новые названия столбцам.

```stata
summarize
ren A n
ren B date
```

```
    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
           A |       120        60.5    34.78505          1        120
           B |         0
       MOBIL |       120    .0161917    .0803075      -.178       .366
      TEXACO |       120    .0119417    .0797036      -.194       .399
         IBM |       120    .0096167     .059024      -.187        .15
-------------+--------------------------------------------------------
         DEC |       120      .01975    .0991438      -.364       .385
      DATGEN |       120    .0074833    .1275399      -.342       .528
       CONED |       120    .0185083    .0502719      -.139       .151
        PSNH |       120   -.0042167    .1094712      -.485       .318
       WEYER |       120    .0096333    .0850664      -.271        .27
-------------+--------------------------------------------------------
       BOISE |       120     .016675    .0974882      -.274       .379
       MOTOR |       120    .0181583    .0972656      -.331        .27
       TANDY |       120    .0250083     .127566      -.246       .454
       PANAM |       120    .0035167    .1318054      -.313       .406
       DELTA |       120    .0116917    .0959317       -.26       .289
-------------+--------------------------------------------------------
      CONTIL |       120      -.0011    .1506992        -.6       .974
      CITCRP |       120    .0118583    .0809719      -.282       .318
      GERBER |       120       .0164    .0877379      -.288       .234
      GENMIL |       120    .0165833    .0650403      -.148        .19
      MARKET |       120    .0139917    .0683532       -.26       .148
-------------+--------------------------------------------------------
      RKFREE |       120    .0068386    .0021869     .00207     .01255
      rkfree |       120    .0068386    .0021869     .00207     .01255

```

Убраем пропущенные значения и создаем новые переменные.

```r
drop if n==.
gen y=MOTOR-RKFREE
gen x=MARKET-RKFREE
```

Визуализируем зависимость.

```r
graph twoway (lfit y x) (scatter y x)
```

Строим модель и проверяем гипотезу об адекватности регрессии.

```r
reg y x
```

Проверим гипотезу ....

```r
test _cons=0 x=1
```

Сделаем предсказание по выборке.

```r
predict u_hat, resid
predict y_hat
```

Протестируем остатки регрессии на нормальность с помощью теста Харке-Бера.

```r
sktest u_hat, noadjust #но все равно не как в R
```

И тест Шапиро-Уилка.

```r
swilk u_hat
```

регрессор - uhat
регрессор - uhat^2
uhat^(n-1) - uhat^(n)
дигональные элементы матрицы шляпницы - для выбросов
