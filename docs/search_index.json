[
["simplereg.html", "Коан 3 Коан о простой линейной регрессии", " Коан 3 Коан о простой линейной регрессии Построим простую линейную регрессию в R и проведем несложные тесты. Загрузим необходимые пакеты и импортируем данные. library(tidyverse) # для манипуляций с данными и построения графиков library(skimr) #для красивого summary library(rio) # для чтения .dta файлов library(car) # для линейных гипотез library(tseries)# для теста на нормальность df = import(file = &quot;us-return.dta&quot;) Исследуем наш датасет. #skim_with(numeric = list(hist = NULL)) skim(df) #посмотрим на наши данные df = na.omit(df) # избавимся от пропущенных значений df = rename(df, n = A, date = B) # дадим столбцам более осмысленные названия :) Будем верить в CAPM :) Оценим параметры модели для компании MOTOR. Соответственно, зависимая переменная - разница доходностей акций MOTOR и безрискового актива, а регрессор - рыночная премия. #создаем новые переменные и добавляем их к набору данных df &lt;- mutate(df, y = MOTOR - RKFREE, x = MARKET - RKFREE) Визуализируем зависимость регрессора и зависимой переменной. ggplot(df, aes(x = x, y = y)) + geom_point() + geom_smooth(method=lm) + labs(x = &quot;risk premium&quot;, y = &quot;return&quot;) Строим нашу модель и проверяем гипотезу об адекватности регрессии. ols &lt;- lm(y ~ x, data = df) summary(ols) Call: lm(formula = y ~ x, data = df) Residuals: Min 1Q Median 3Q Max -0.168421 -0.059381 -0.003399 0.061373 0.182991 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.005253 0.007200 0.730 0.467 x 0.848150 0.104814 8.092 5.91e-13 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.07844 on 118 degrees of freedom Multiple R-squared: 0.3569, Adjusted R-squared: 0.3514 F-statistic: 65.48 on 1 and 118 DF, p-value: 5.913e-13 Проверим гипотезу о равенстве коэффициента при регрессии единице. linearHypothesis(ols, c(&quot;(Intercept) = 0&quot;, &quot;x = 1&quot;)) Linear hypothesis test Hypothesis: (Intercept) = 0 x = 1 Model 1: restricted model Model 2: y ~ x Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 120 0.74108 2 118 0.72608 2 0.014998 1.2187 0.2993 Сделаем предсказание по выборке. df &lt;- mutate(df, u_hat = resid(ols), y_hat = predict(ols), n = seq(dim(df)[1])) Посмотрим на остатки :) Протестируем остатки регрессии на нормальность с помощью теста Харке-Бера. jarque.bera.test(df$u_hat) ##в R нет поправки на размер выборки Jarque Bera Test data: df$u_hat X-squared = 1.7803, df = 2, p-value = 0.4106 И тест Шапиро-Уилка. shapiro.test(df$u_hat) Shapiro-Wilk normality test data: df$u_hat W = 0.99021, p-value = 0.5531 Оба теста указывают на нормальность распределения остатков регрессии. 3.0.0.1 То же самое в стате Загружаем данные, любуемся и даем новые названия столбцам. use us-return.dta summarize ren A n ren B date Убраем пропущенные значения и создаем новые переменные. drop if n==. gen y=MOTOR-RKFREE gen x=MARKET-RKFREE Визуализируем зависимость. graph twoway (lfit y x) (scatter y x) Строим модель и проверяем гипотезу об адекватности регрессии. reg y x Проверим гипотезу …. test _cons=0 x=1 Сделаем предсказание по выборке. predict u_hat, resid predict y_hat Протестируем остатки регрессии на нормальность с помощью теста Харке-Бера. sktest u_hat, noadjust #но все равно не как в R И тест Шапиро-Уилка. swilk u_hat "]
]
